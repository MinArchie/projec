{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ik6bnTKdaL",
        "outputId": "9b96e6b6-5235-41c4-d588-3395fa2c86f1"
      },
      "outputs": [],
      "source": [
        "# !pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upb1ujAzpdoF",
        "outputId": "07f4fd66-e596-44f3-e278-863fd7c1ee57"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCuL3_2D0Qtt",
        "outputId": "26acf2cf-1e00-4721-eef6-e2a2e35c7d87"
      },
      "outputs": [],
      "source": [
        "# !ls /content/drive/MyDrive/all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9IcqbTYyDLM7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from torch_geometric.data import Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T52h1sBJpodi"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# df= pd.read_csv(\"/content/drive/My Drive/all_data/twosides.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FDIURagpp6tB",
        "outputId": "8d6f337c-992c-42e2-ea36-fa802fabb1a7"
      },
      "outputs": [],
      "source": [
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRu56our0Rra",
        "outputId": "bd84468e-5e2e-4bf8-c694-cf0652c40fce"
      },
      "outputs": [],
      "source": [
        "# !pip install PyTDC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGAQ9ygA4Esy",
        "outputId": "f4d14767-cdae-47ea-e529-0e880a1e088c"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UCnhr4At4v0q",
        "outputId": "1a8f63e2-2b81-48a1-de39-1298b986eae0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Drug1_ID</th>\n",
              "      <th>Drug1</th>\n",
              "      <th>Drug2_ID</th>\n",
              "      <th>Drug2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DB04571</td>\n",
              "      <td>CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1</td>\n",
              "      <td>DB00460</td>\n",
              "      <td>COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DB00855</td>\n",
              "      <td>NCC(=O)CCC(O)=O</td>\n",
              "      <td>DB00460</td>\n",
              "      <td>COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DB09536</td>\n",
              "      <td>O=[Ti]=O</td>\n",
              "      <td>DB00460</td>\n",
              "      <td>COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DB01600</td>\n",
              "      <td>CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1</td>\n",
              "      <td>DB00460</td>\n",
              "      <td>COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DB09000</td>\n",
              "      <td>CC(CN(C)C)CN1C2=CC=CC=C2SC2=C1C=C(C=C2)C#N</td>\n",
              "      <td>DB00460</td>\n",
              "      <td>COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191803</th>\n",
              "      <td>DB00437</td>\n",
              "      <td>OC1=NC=NC2=C1C=NN2</td>\n",
              "      <td>DB00492</td>\n",
              "      <td>CCC(=O)O[C@@H](O[P@](=O)(CCCCC1=CC=CC=C1)CC(=O...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191804</th>\n",
              "      <td>DB00437</td>\n",
              "      <td>OC1=NC=NC2=C1C=NN2</td>\n",
              "      <td>DB09477</td>\n",
              "      <td>[H][C@@](C)(N[C@@]([H])(CCC1=CC=CC=C1)C(O)=O)C...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191805</th>\n",
              "      <td>DB00437</td>\n",
              "      <td>OC1=NC=NC2=C1C=NN2</td>\n",
              "      <td>DB00790</td>\n",
              "      <td>[H][C@]12C[C@H](N(C(=O)[C@H](C)N[C@@H](CCC)C(=...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191806</th>\n",
              "      <td>DB00415</td>\n",
              "      <td>[H][C@]12SC(C)(C)[C@@H](N1C(=O)[C@H]2NC(=O)[C@...</td>\n",
              "      <td>DB00437</td>\n",
              "      <td>OC1=NC=NC2=C1C=NN2</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191807</th>\n",
              "      <td>DB00437</td>\n",
              "      <td>OC1=NC=NC2=C1C=NN2</td>\n",
              "      <td>DB00691</td>\n",
              "      <td>CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1C...</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191808 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Drug1_ID                                              Drug1 Drug2_ID  \\\n",
              "0       DB04571                CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00460   \n",
              "1       DB00855                                    NCC(=O)CCC(O)=O  DB00460   \n",
              "2       DB09536                                           O=[Ti]=O  DB00460   \n",
              "3       DB01600              CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1  DB00460   \n",
              "4       DB09000         CC(CN(C)C)CN1C2=CC=CC=C2SC2=C1C=C(C=C2)C#N  DB00460   \n",
              "...         ...                                                ...      ...   \n",
              "191803  DB00437                                 OC1=NC=NC2=C1C=NN2  DB00492   \n",
              "191804  DB00437                                 OC1=NC=NC2=C1C=NN2  DB09477   \n",
              "191805  DB00437                                 OC1=NC=NC2=C1C=NN2  DB00790   \n",
              "191806  DB00415  [H][C@]12SC(C)(C)[C@@H](N1C(=O)[C@H]2NC(=O)[C@...  DB00437   \n",
              "191807  DB00437                                 OC1=NC=NC2=C1C=NN2  DB00691   \n",
              "\n",
              "                                                    Drug2   Y  \n",
              "0       COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...   1  \n",
              "1       COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...   1  \n",
              "2       COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...   1  \n",
              "3       COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...   1  \n",
              "4       COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...   1  \n",
              "...                                                   ...  ..  \n",
              "191803  CCC(=O)O[C@@H](O[P@](=O)(CCCCC1=CC=CC=C1)CC(=O...  86  \n",
              "191804  [H][C@@](C)(N[C@@]([H])(CCC1=CC=CC=C1)C(O)=O)C...  86  \n",
              "191805  [H][C@]12C[C@H](N(C(=O)[C@H](C)N[C@@H](CCC)C(=...  86  \n",
              "191806                                 OC1=NC=NC2=C1C=NN2  86  \n",
              "191807  CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1C...  86  \n",
              "\n",
              "[191808 rows x 5 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_drugbank = pd.read_csv(\"./datasets/drugbank.csv\")\n",
        "df_drugbank  # Display first few rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "d3nL2-G3M-wa",
        "outputId": "f205ee27-45d1-4bf7-877f-7d7d9c698f38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Drug1_ID    0\n",
              "Drug1       0\n",
              "Drug2_ID    0\n",
              "Drug2       0\n",
              "Y           0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_drugbank.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eLgMXDH-da79"
      },
      "outputs": [],
      "source": [
        "from operator import index\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K6-ya_WOda-b"
      },
      "outputs": [],
      "source": [
        "def one_of_k_encoding(k, possible_values):\n",
        "    if k not in possible_values:\n",
        "        raise ValueError(f\"{k} is not a valid value in {possible_values}\")\n",
        "    return [k == e for e in possible_values]\n",
        "\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "J9dy_v5OfI4L"
      },
      "outputs": [],
      "source": [
        "def save_data(data, filename, dirname=\"data/preprocessed\", dataset=\"drugbank\"):\n",
        "    save_path = os.path.join(dirname, dataset)\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    filepath = os.path.join(save_path, filename)\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    print(f'\\nData saved as {filepath}!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psa37934dbAy"
      },
      "outputs": [],
      "source": [
        "def atom_features(atom, atom_symbols, explicit_H=True, use_chirality=False):\n",
        "\n",
        "    results = one_of_k_encoding_unk(atom.GetSymbol(), atom_symbols + ['Unknown']) + \\\n",
        "            one_of_k_encoding(atom.GetDegree(),[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) + \\\n",
        "            one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) + \\\n",
        "                [atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \\\n",
        "            one_of_k_encoding_unk(atom.GetHybridization(), [\n",
        "                Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
        "                Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.\n",
        "                                    SP3D, Chem.rdchem.HybridizationType.SP3D2\n",
        "                ]) + [atom.GetIsAromatic()]\n",
        "    # In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`\n",
        "    if explicit_H:\n",
        "        results = results + one_of_k_encoding_unk(atom.GetTotalNumHs(),\n",
        "                                                [0, 1, 2, 3, 4])\n",
        "    # if use_chirality:\n",
        "    #     try:\n",
        "    #         results = results + one_of_k_encoding_unk(\n",
        "    #         atom.GetProp('_CIPCode'),\n",
        "    #         ['R', 'S']) + [atom.HasProp('_ChiralityPossible')]\n",
        "    #     except:\n",
        "    #         results = results + [False, False\n",
        "    #                         ] + [atom.HasProp('_ChiralityPossible')]\n",
        "\n",
        "    results = np.array(results).astype(np.float32)\n",
        "\n",
        "    return torch.from_numpy(results)\n",
        "\n",
        "\n",
        "def edge_features(bond):\n",
        "    bond_type = bond.GetBondType()\n",
        "    return torch.tensor([\n",
        "        bond_type == Chem.rdchem.BondType.SINGLE,\n",
        "        bond_type == Chem.rdchem.BondType.DOUBLE,\n",
        "        bond_type == Chem.rdchem.BondType.TRIPLE,\n",
        "        bond_type == Chem.rdchem.BondType.AROMATIC,\n",
        "        bond.GetIsConjugated(),\n",
        "        bond.IsInRing()]).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-vSP9taBdbDK"
      },
      "outputs": [],
      "source": [
        "def generate_drug_data(mol_graph, atom_symbols):\n",
        "\n",
        "    edge_list = torch.LongTensor([(b.GetBeginAtomIdx(), b.GetEndAtomIdx(), *edge_features(b)) for b in mol_graph.GetBonds()])\n",
        "    edge_list, edge_feats = (edge_list[:, :2], edge_list[:, 2:].float()) if len(edge_list) else (torch.LongTensor([]), torch.FloatTensor([]))\n",
        "    edge_list = torch.cat([edge_list, edge_list[:, [1, 0]]], dim=0) if len(edge_list) else edge_list\n",
        "    edge_feats = torch.cat([edge_feats]*2, dim=0) if len(edge_feats) else edge_feats\n",
        "\n",
        "    features = [(atom.GetIdx(), atom_features(atom, atom_symbols)) for atom in mol_graph.GetAtoms()]\n",
        "    features.sort()\n",
        "    _, features = zip(*features)\n",
        "    features = torch.stack(features)\n",
        "\n",
        "    line_graph_edge_index = torch.LongTensor([])\n",
        "    if edge_list.nelement() != 0:\n",
        "        conn = (edge_list[:, 1].unsqueeze(1) == edge_list[:, 0].unsqueeze(0)) & (edge_list[:, 0].unsqueeze(1) != edge_list[:, 1].unsqueeze(0))\n",
        "        line_graph_edge_index = conn.nonzero(as_tuple=False).T\n",
        "\n",
        "    new_edge_index = edge_list.T\n",
        "\n",
        "    return features, new_edge_index, edge_feats, line_graph_edge_index\n",
        "\n",
        "\n",
        "def load_drug_mol_data(df_drugbank):\n",
        "\n",
        "    data = df_drugbank\n",
        "    drug_id_mol_tup = []\n",
        "    symbols = list()\n",
        "    drug_smile_dict = {}\n",
        "\n",
        "    for id1, smiles1, id2, smiles2, relation in zip(data['Drug1_ID'], data['Drug1'], data['Drug2_ID'], data['Drug2'], data['Y']):\n",
        "        drug_smile_dict[id1] = smiles1\n",
        "        drug_smile_dict[id2] = smiles2\n",
        "\n",
        "    for id, smiles in drug_smile_dict.items():\n",
        "        mol =  Chem.MolFromSmiles(smiles.strip())\n",
        "        if mol is not None:\n",
        "            drug_id_mol_tup.append((id, mol))\n",
        "            symbols.extend(atom.GetSymbol() for atom in mol.GetAtoms())\n",
        "\n",
        "    symbols = list(set(symbols))\n",
        "    drug_data = {id: generate_drug_data(mol, symbols) for id, mol in tqdm(drug_id_mol_tup, desc='Processing drugs')}\n",
        "    save_data(drug_data, 'drug_data.pkl', dirname=\"data/preprocessed\", dataset=\"drugbank\")\n",
        "    return drug_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_olhIPP6dbFj"
      },
      "outputs": [],
      "source": [
        "def generate_pair_triplets(df_drugbank, neg_ent =1, seed=42, dirname=\"data/preprocessed\", dataset=\"drugbank\"):\n",
        "    pos_triplets = []\n",
        "    drug_ids = []\n",
        "\n",
        "    with open(f'{dirname}/{dataset.lower()}/drug_data.pkl', 'rb') as f:\n",
        "        drug_ids = list(pickle.load(f).keys())\n",
        "        data = df_drugbank\n",
        "    for id1, id2, relation in zip(data['Drug1_ID'], data['Drug2_ID'],  data['Y']):\n",
        "        if ((id1 not in drug_ids) or (id2 not in drug_ids)): continue\n",
        "        # Drugbank dataset is 1-based index, need to substract by 1\n",
        "        if dataset in ('drugbank', ):\n",
        "            relation -= 1\n",
        "        pos_triplets.append([id1, id2, relation])\n",
        "\n",
        "    if len(pos_triplets) == 0:\n",
        "        raise ValueError('All tuples are invalid.')\n",
        "\n",
        "    pos_triplets = np.array(pos_triplets)\n",
        "    data_statistics = load_data_statistics(pos_triplets)\n",
        "    drug_ids = np.array(drug_ids)\n",
        "\n",
        "    random_state = np.random.RandomState(seed)\n",
        "\n",
        "    neg_samples = []\n",
        "    for pos_item in tqdm(pos_triplets, desc='Generating Negative sample'):\n",
        "        temp_neg = []\n",
        "        h, t, r = pos_item[:3]\n",
        "\n",
        "        if dataset == 'drugbank':\n",
        "            neg_heads, neg_tails = _normal_batch(h, t, r, neg_ent, data_statistics, drug_ids, random_state)\n",
        "            temp_neg = [str(neg_h) + '$h' for neg_h in neg_heads] + \\\n",
        "                        [str(neg_t) + '$t' for neg_t in neg_tails]\n",
        "        else:\n",
        "            existing_drug_ids = np.asarray(list(set(\n",
        "                np.concatenate([data_statistics[\"ALL_TRUE_T_WITH_HR\"][(h, r)], data_statistics[\"ALL_TRUE_H_WITH_TR\"][(h, r)]], axis=0)\n",
        "                )))\n",
        "            temp_neg = _corrupt_ent(existing_drug_ids, neg_ent, drug_ids, random_state)\n",
        "\n",
        "        neg_samples.append('_'.join(map(str, temp_neg[:neg_ent])))\n",
        "\n",
        "    df = pd.DataFrame({'Drug1_ID': pos_triplets[:, 0],\n",
        "                        'Drug2_ID': pos_triplets[:, 1],\n",
        "                        'Y': pos_triplets[:, 2],\n",
        "                        'Neg samples': neg_samples})\n",
        "    filename = f'{dirname}/{dataset}/pair_pos_neg_triplets.csv'\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f'\\nData saved as {filename}!')\n",
        "    save_data(data_statistics, 'data_statistics.pkl', dirname=\"data/preprocessed\", dataset=\"drugbank\")\n",
        "\n",
        "\n",
        "def load_data_statistics(all_tuples):\n",
        "\n",
        "    print('Loading data statistics ...')\n",
        "    statistics = dict()\n",
        "    statistics[\"ALL_TRUE_H_WITH_TR\"] = defaultdict(list)\n",
        "    statistics[\"ALL_TRUE_T_WITH_HR\"] = defaultdict(list)\n",
        "    statistics[\"FREQ_REL\"] = defaultdict(int)\n",
        "    statistics[\"ALL_H_WITH_R\"] = defaultdict(dict)\n",
        "    statistics[\"ALL_T_WITH_R\"] = defaultdict(dict)\n",
        "    statistics[\"ALL_TAIL_PER_HEAD\"] = {}\n",
        "    statistics[\"ALL_HEAD_PER_TAIL\"] = {}\n",
        "\n",
        "    for h, t, r in tqdm(all_tuples, desc='Getting data statistics'):\n",
        "        statistics[\"ALL_TRUE_H_WITH_TR\"][(t, r)].append(h)\n",
        "        statistics[\"ALL_TRUE_T_WITH_HR\"][(h, r)].append(t)\n",
        "        statistics[\"FREQ_REL\"][r] += 1.0\n",
        "        statistics[\"ALL_H_WITH_R\"][r][h] = 1\n",
        "        statistics[\"ALL_T_WITH_R\"][r][t] = 1\n",
        "\n",
        "    for t, r in statistics[\"ALL_TRUE_H_WITH_TR\"]:\n",
        "        statistics[\"ALL_TRUE_H_WITH_TR\"][(t, r)] = np.array(list(set(statistics[\"ALL_TRUE_H_WITH_TR\"][(t, r)])))\n",
        "    for h, r in statistics[\"ALL_TRUE_T_WITH_HR\"]:\n",
        "        statistics[\"ALL_TRUE_T_WITH_HR\"][(h, r)] = np.array(list(set(statistics[\"ALL_TRUE_T_WITH_HR\"][(h, r)])))\n",
        "\n",
        "    for r in statistics[\"FREQ_REL\"]:\n",
        "        statistics[\"ALL_H_WITH_R\"][r] = np.array(list(statistics[\"ALL_H_WITH_R\"][r].keys()))\n",
        "        statistics[\"ALL_T_WITH_R\"][r] = np.array(list(statistics[\"ALL_T_WITH_R\"][r].keys()))\n",
        "        statistics[\"ALL_HEAD_PER_TAIL\"][r] = statistics[\"FREQ_REL\"][r] / len(statistics[\"ALL_T_WITH_R\"][r])\n",
        "        statistics[\"ALL_TAIL_PER_HEAD\"][r] = statistics[\"FREQ_REL\"][r] / len(statistics[\"ALL_H_WITH_R\"][r])\n",
        "\n",
        "    print('getting data statistics done!')\n",
        "\n",
        "    return statistics\n",
        "\n",
        "\n",
        "def _corrupt_ent(positive_existing_ents, max_num, drug_ids, random_state):\n",
        "    corrupted_ents = []\n",
        "    while len(corrupted_ents) < max_num:\n",
        "        candidates = random_state.choice(drug_ids, (max_num - len(corrupted_ents)) * 2, replace=False)\n",
        "        invalid_drug_ids = np.concatenate([positive_existing_ents, corrupted_ents], axis=0)\n",
        "        mask = np.isin(candidates, invalid_drug_ids, assume_unique=True, invert=True)\n",
        "        corrupted_ents.extend(candidates[mask])\n",
        "\n",
        "    corrupted_ents = np.array(corrupted_ents)[:max_num]\n",
        "    return corrupted_ents\n",
        "\n",
        "\n",
        "def _normal_batch( h, t, r, neg_size, data_statistics, drug_ids, random_state):\n",
        "    neg_size_h = 0\n",
        "    neg_size_t = 0\n",
        "    prob = data_statistics[\"ALL_TAIL_PER_HEAD\"][r] / (data_statistics[\"ALL_TAIL_PER_HEAD\"][r] +\n",
        "                                                            data_statistics[\"ALL_HEAD_PER_TAIL\"][r])\n",
        "    # prob = 2\n",
        "    for i in range(neg_size):\n",
        "        if random_state.random() < prob:\n",
        "            neg_size_h += 1\n",
        "        else:\n",
        "            neg_size_t +=1\n",
        "\n",
        "    return (_corrupt_ent(data_statistics[\"ALL_TRUE_H_WITH_TR\"][t, r], neg_size_h, drug_ids, random_state),\n",
        "            _corrupt_ent(data_statistics[\"ALL_TRUE_T_WITH_HR\"][h, r], neg_size_t, drug_ids, random_state))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PTAVZkFsdbIK"
      },
      "outputs": [],
      "source": [
        "def split_data(class_name, seed, test_ratio, n_folds, dirname=\"data/preprocessed\", dataset=\"drugbank\"):\n",
        "    filename = f'{dirname}/{dataset}/pair_pos_neg_triplets.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "    seed = seed\n",
        "    class_name = class_name\n",
        "    test_size_ratio = test_ratio\n",
        "    n_folds = n_folds\n",
        "    save_to_filename = os.path.splitext(filename)[0]\n",
        "    cv_split = StratifiedShuffleSplit(n_splits=n_folds, test_size=test_size_ratio, random_state=seed)\n",
        "    for fold_i, (train_index, test_index) in enumerate(cv_split.split(X=df, y=df[class_name])):\n",
        "        print(f'Fold {fold_i} generated!')\n",
        "        train_df = df.iloc[train_index]\n",
        "        test_df = df.iloc[test_index]\n",
        "        train_df.to_csv(f'{save_to_filename}_train_fold{fold_i}.csv', index=False)\n",
        "        print(f'{save_to_filename}_train_fold{fold_i}.csv', 'saved!')\n",
        "        test_df.to_csv(f'{save_to_filename}_test_fold{fold_i}.csv', index=False)\n",
        "        print(f'{save_to_filename}_test_fold{fold_i}.csv', 'saved!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIycpkW0dbKi",
        "outputId": "fa31fc18-1dda-4ef4-d51d-a39d73c71a77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[10:31:46] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
            "[10:31:46] SMILES Parse Error: check for mistakes around position 76:\n",
            "[10:31:46] C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C\n",
            "[10:31:46] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[10:31:46] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n",
            "Processing drugs:   3%|▎         | 57/1705 [00:00<00:02, 569.41it/s]C:\\Users\\swath\\AppData\\Local\\Temp\\ipykernel_2736\\287951120.py:18: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3729.)\n",
            "  new_edge_index = edge_list.T\n",
            "Processing drugs: 100%|██████████| 1705/1705 [00:03<00:00, 567.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data saved as data/preprocessed\\drugbank\\drug_data.pkl!\n"
          ]
        }
      ],
      "source": [
        "drug_data = load_drug_mol_data(df_drugbank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl1cnRfFjL9b",
        "outputId": "293319f2-7841-4301-e9ac-886219cf49e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data statistics ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Getting data statistics: 100%|██████████| 191798/191798 [00:00<00:00, 439621.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "getting data statistics done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Negative sample: 100%|██████████| 191798/191798 [00:17<00:00, 11137.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data saved as data/preprocessed/drugbank/pair_pos_neg_triplets.csv!\n",
            "\n",
            "Data saved as data/preprocessed\\drugbank\\data_statistics.pkl!\n"
          ]
        }
      ],
      "source": [
        "generate_pair_triplets(df_drugbank, neg_ent=1, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opVZN-_JdbMy",
        "outputId": "cc414afd-204e-4279-c878-e9ada0126b6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0 generated!\n",
            "data/preprocessed/drugbank/pair_pos_neg_triplets_train_fold0.csv saved!\n",
            "data/preprocessed/drugbank/pair_pos_neg_triplets_test_fold0.csv saved!\n",
            "Fold 1 generated!\n",
            "data/preprocessed/drugbank/pair_pos_neg_triplets_train_fold1.csv saved!\n",
            "data/preprocessed/drugbank/pair_pos_neg_triplets_test_fold1.csv saved!\n",
            "Fold 2 generated!\n",
            "data/preprocessed/drugbank/pair_pos_neg_triplets_train_fold2.csv saved!\n",
            "data/preprocessed/drugbank/pair_pos_neg_triplets_test_fold2.csv saved!\n"
          ]
        }
      ],
      "source": [
        "split_data('Y', seed=42, test_ratio=0.2, n_folds=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLlAhIkWmv_L"
      },
      "source": [
        "# getting datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qOplVp9TdbPU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Batch, Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zf9ggSdBl4Kr"
      },
      "outputs": [],
      "source": [
        "NUM_FEATURES = None\n",
        "NUM_EDGE_FEATURES = None\n",
        "bipartite_edge_dict = dict()\n",
        "drug_num_node_indices = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "o-aibDxUl4NN"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def total_num_rel():\n",
        "    \"\"\"Returns the total number of relations for DrugBank dataset.\"\"\"\n",
        "    return 86\n",
        "\n",
        "def split_train_valid(data, fold, val_ratio=0.2):\n",
        "    \"\"\"Splits the dataset into training and validation sets.\"\"\"\n",
        "    cv_split = StratifiedShuffleSplit(n_splits=2, test_size=val_ratio, random_state=fold)\n",
        "    pos_triplets, neg_samples = data\n",
        "    train_index, val_index = next(iter(cv_split.split(X=pos_triplets, y=pos_triplets[:, 2])))\n",
        "\n",
        "    train_tup = (pos_triplets[train_index], neg_samples[train_index])\n",
        "    val_tup = (pos_triplets[val_index], neg_samples[val_index])\n",
        "\n",
        "    return train_tup, val_tup\n",
        "\n",
        "def load_ddi_data_fold(fold, batch_size=32, data_size_ratio=1.0, valid_ratio=0.2, dirname=\"data/preprocessed\"):\n",
        "    \"\"\"Loads DrugBank dataset and prepares PyTorch DataLoaders.\"\"\"\n",
        "    global NUM_FEATURES, NUM_EDGE_FEATURES, drug_num_node_indices\n",
        "\n",
        "    dataset_name = \"drugbank\"\n",
        "    print(f'Loading {dataset_name}...')\n",
        "\n",
        "    # Load processed drug data\n",
        "    drug_data_file = f'{dirname}/{dataset_name}/drug_data.pkl'\n",
        "    print('\\nLoading processed drug data...')\n",
        "    with open(drug_data_file, 'rb') as f:\n",
        "        all_drug_data = pickle.load(f)\n",
        "\n",
        "    # Extract feature dimensions\n",
        "    NUM_FEATURES, _, NUM_EDGE_FEATURES = next(iter(all_drug_data.values()))[:3]\n",
        "    NUM_FEATURES, NUM_EDGE_FEATURES = NUM_FEATURES.shape[1], NUM_EDGE_FEATURES.shape[1]\n",
        "\n",
        "    # Convert data to CustomData format\n",
        "    all_drug_data = {\n",
        "        drug_id: CustomData(x=data[0], edge_index=data[1], edge_feats=data[2], line_graph_edge_index=data[3])\n",
        "        for drug_id, data in all_drug_data.items()\n",
        "    }\n",
        "\n",
        "    # Speed up training with precomputed indices\n",
        "    drug_num_node_indices = {\n",
        "        drug_id: torch.zeros(data.x.size(0)).long() for drug_id, data in all_drug_data.items()\n",
        "    }\n",
        "\n",
        "    # Load train/validation/test splits\n",
        "    train_tup = load_split(f'train_fold{fold}', dirname)\n",
        "    train_tup, val_tup = split_train_valid(train_tup, fold)\n",
        "    test_tup = load_split(f'test_fold{fold}', dirname)\n",
        "\n",
        "    print(f'{train_tup[1].shape[1]} negative samples on fold {fold}')\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_data = DrugDataset(train_tup, all_drug_data, seed=fold)\n",
        "    val_data = DrugDataset(val_tup, all_drug_data, seed=fold)\n",
        "    test_data = DrugDataset(test_tup, all_drug_data, seed=fold)\n",
        "\n",
        "    print(f\"\\nTraining on {len(train_data)} samples, validating on {len(val_data)}, and testing on {len(test_data)} samples.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DrugDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DrugDataLoader(val_data, batch_size=batch_size)\n",
        "    test_loader = DrugDataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, NUM_FEATURES, NUM_EDGE_FEATURES\n",
        "\n",
        "def load_split(split_name, dirname=\"data/preprocessed\"):\n",
        "    \"\"\"Loads dataset splits for DrugBank.\"\"\"\n",
        "    filename = f'{dirname}/drugbank/pair_pos_neg_triplets_{split_name}.csv'\n",
        "    print(f'\\nLoading {filename}...')\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    pos_triplets = [(d1, d2, r) for d1, d2, r in zip(df['Drug1_ID'], df['Drug2_ID'], df['Y'])]\n",
        "    neg_samples = [[str(e) for e in neg_s.split('_')] for neg_s in df['Neg samples']]\n",
        "\n",
        "    return np.array(pos_triplets), np.array(neg_samples)\n",
        "\n",
        "# ===========================\n",
        "# Dataset Classes\n",
        "# ===========================\n",
        "\n",
        "class DrugDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for DrugBank.\"\"\"\n",
        "\n",
        "    def __init__(self, pos_neg_tuples, all_drug_data, ratio=1.0, seed=0):\n",
        "        self.pair_triplets = []\n",
        "        self.ratio = ratio\n",
        "        self.drug_ids = list(all_drug_data.keys())\n",
        "        self.all_drug_data = all_drug_data\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "\n",
        "        for pos_item, neg_list in zip(*pos_neg_tuples):\n",
        "            if (pos_item[0] in self.drug_ids) and (pos_item[1] in self.drug_ids):\n",
        "                self.pair_triplets.append((pos_item, neg_list))\n",
        "\n",
        "        if ratio != 1.0:\n",
        "            self.rng.shuffle(self.pair_triplets)\n",
        "            limit = math.ceil(len(self.pair_triplets) * ratio)\n",
        "            self.pair_triplets = self.pair_triplets[:limit]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        old_id_to_new_batch_id = {}\n",
        "        batch_drug_feats = []\n",
        "        self.node_ind_seqs = []\n",
        "        self.node_i_ind_seqs_for_pair = []\n",
        "        self.node_j_ind_seqs_for_pair = []\n",
        "\n",
        "        combo_indices_pos = []\n",
        "        combo_indices_neg = []\n",
        "        already_in_combo = {}\n",
        "        rels = []\n",
        "        batch_unique_pairs= []\n",
        "\n",
        "        for ind, (pos_item, neg_list) in enumerate(batch):\n",
        "            h, t, r = pos_item[:3]\n",
        "            idx_h, h_num_nodes = self._get_new_batch_id_and_num_nodes(h, old_id_to_new_batch_id, batch_drug_feats)\n",
        "            idx_t, t_num_nodes = self._get_new_batch_id_and_num_nodes(t, old_id_to_new_batch_id, batch_drug_feats)\n",
        "            combo_idx = self._get_combo_index((idx_h, idx_t), (h, t), already_in_combo, batch_unique_pairs, (h_num_nodes, t_num_nodes))\n",
        "            combo_indices_pos.append(combo_idx)\n",
        "\n",
        "            rels.append(int(r))\n",
        "\n",
        "            for neg_s in neg_list:\n",
        "                s = neg_s.split('$')\n",
        "                neg_idx, neg_num_nodes = self._get_new_batch_id_and_num_nodes(s[0], old_id_to_new_batch_id, batch_drug_feats)\n",
        "                if ('h' == s[1].lower()):\n",
        "                        combo_idx = self._get_combo_index((neg_idx, idx_t), (s[0], t), already_in_combo, batch_unique_pairs, (neg_num_nodes, t_num_nodes))\n",
        "                else:\n",
        "                    combo_idx = self._get_combo_index((idx_h, neg_idx), (h, s[0]), already_in_combo, batch_unique_pairs, (h_num_nodes, neg_num_nodes))\n",
        "\n",
        "                combo_indices_neg.append(combo_idx)\n",
        "\n",
        "        batch_drug_data = Batch.from_data_list(batch_drug_feats, follow_batch=['edge_index'])\n",
        "        batch_drug_pair_indices = torch.LongTensor(combo_indices_pos + combo_indices_neg)\n",
        "        batch_unique_drug_pair = Batch.from_data_list(batch_unique_pairs, follow_batch=['edge_index'])\n",
        "        node_j_for_pairs = torch.cat(self.node_j_ind_seqs_for_pair)\n",
        "        node_i_for_pairs = torch.cat(self.node_i_ind_seqs_for_pair)\n",
        "        rels = torch.LongTensor(rels)\n",
        "\n",
        "        return batch_drug_data, batch_unique_drug_pair, rels, batch_drug_pair_indices, node_j_for_pairs, node_i_for_pairs\n",
        "\n",
        "    def _get_new_batch_id_and_num_nodes(self, old_id, old_id_to_new_batch_id, batch_drug_feats):\n",
        "        new_id = old_id_to_new_batch_id.get(old_id, -1)\n",
        "        num_nodes = self.all_drug_data[old_id].x.size(0)\n",
        "        if new_id == - 1:\n",
        "            new_id = len(old_id_to_new_batch_id)\n",
        "            old_id_to_new_batch_id[old_id] = new_id\n",
        "            batch_drug_feats.append(self.all_drug_data[old_id])\n",
        "            start = (self.node_ind_seqs[-1][-1] + 1) if len(self.node_ind_seqs) else 0\n",
        "            self.node_ind_seqs.append(torch.arange(num_nodes) + start)\n",
        "\n",
        "        return new_id, num_nodes\n",
        "\n",
        "    def _get_combo_index(self, combo, old_combo, already_in_combo, unique_pairs, num_nodes):\n",
        "        idx = already_in_combo.get(combo, -1)\n",
        "        if idx == -1:\n",
        "            idx = len(already_in_combo)\n",
        "            already_in_combo[combo] = idx\n",
        "            pair_edge_index = bipartite_edge_dict.get(old_combo)\n",
        "            if pair_edge_index is None:\n",
        "                index_j = torch.arange(num_nodes[0]).repeat_interleave(num_nodes[1])\n",
        "                index_i = torch.arange(num_nodes[1]).repeat(num_nodes[0])\n",
        "                pair_edge_index = torch.stack([index_j, index_i])\n",
        "                bipartite_edge_dict[old_combo] = pair_edge_index\n",
        "\n",
        "            j_num_indices, i_num_indices = drug_num_node_indices[old_combo[0]], drug_num_node_indices[old_combo[1]]\n",
        "            unique_pairs.append(PairData(j_num_indices, i_num_indices, pair_edge_index))\n",
        "            self.node_j_ind_seqs_for_pair.append(self.node_ind_seqs[combo[0]])\n",
        "            self.node_i_ind_seqs_for_pair.append(self.node_ind_seqs[combo[1]])\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pair_triplets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.pair_triplets[index]\n",
        "\n",
        "class DrugDataLoader(DataLoader):\n",
        "    \"\"\"Custom DataLoader for DrugBank.\"\"\"\n",
        "    def __init__(self, data, **kwargs):\n",
        "        super().__init__(data, collate_fn=data.collate_fn, **kwargs)\n",
        "\n",
        "class PairData(Data):\n",
        "\n",
        "    def __init__(self, j_indices, i_indices, pair_edge_index):\n",
        "        super().__init__()\n",
        "        self.i_indices = i_indices\n",
        "        self.j_indices = j_indices\n",
        "        self.edge_index = pair_edge_index\n",
        "        self.num_nodes = None\n",
        "\n",
        "    def __inc__(self, key, value, *args, **kwargs):\n",
        "    # In case of \"TypeError: __inc__() takes 3 positional arguments but 4 were given\"\n",
        "    # Replace with \"def __inc__(self, key, value, *args, **kwargs)\"\n",
        "        if key == 'edge_index':\n",
        "            return torch.tensor([[self.j_indices.shape[0]], [self.i_indices.shape[0]]])\n",
        "        if key in ('i_indices', 'j_indices'):\n",
        "            return 1\n",
        "        return super().__inc__(self, key, value, args, kwargs)\n",
        "            # In case of \"TypeError: __inc__() takes 3 positional arguments but 4 were given\"\n",
        "            # Replace with \"return super().__inc__(self, key, value, args, kwargs)\"\n",
        "\n",
        "\n",
        "\n",
        "class CustomData(Data):\n",
        "    def __inc__(self, key, value, *args, **kwargs):  # Accepts additional arguments\n",
        "        if key == 'line_graph_edge_index':\n",
        "            return self.edge_index.size(1) if self.edge_index.nelement() != 0 else 0\n",
        "        return super().__inc__(key, value, *args, **kwargs)  # Pass extra args\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S27eWxSrl4Rz",
        "outputId": "e2c1b9d8-915e-415c-9d2d-61035d311bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading drugbank...\n",
            "\n",
            "Loading processed drug data...\n",
            "\n",
            "Loading data/preprocessed/drugbank/pair_pos_neg_triplets_train_fold0.csv...\n",
            "\n",
            "Loading data/preprocessed/drugbank/pair_pos_neg_triplets_test_fold0.csv...\n",
            "1 negative samples on fold 0\n",
            "\n",
            "Training on 122750 samples, validating on 30688, and testing on 38360 samples.\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader, test_loader, num_features, num_edge_features = load_ddi_data_fold(\n",
        "    fold=0, batch_size=32, data_size_ratio=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZyNu5g4qM7T"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B2s2GmAMqnLF",
        "outputId": "c7b2abee-646c-4620-f735-a5bb8364dab1"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "# !pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0EWh7ANuqOwC"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch_geometric.nn.inits import glorot\n",
        "from torch_geometric.utils import degree\n",
        "from torch_scatter import scatter\n",
        "\n",
        "# Define CustomDropout\n",
        "class CustomDropout(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        self.dropout = (lambda x: x) if p == 0 else nn.Dropout(p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.dropout(input)\n",
        "\n",
        "# Define GmpnnBlock\n",
        "class GmpnnBlock(nn.Module):\n",
        "    def __init__(self, edge_feats, n_feats, n_iter, dropout):\n",
        "        super().__init__()\n",
        "        self.n_feats = n_feats\n",
        "        self.n_iter = n_iter\n",
        "        self.dropout = dropout\n",
        "        self.snd_n_feats = n_feats * 2\n",
        "\n",
        "        self.w_i = nn.Parameter(torch.Tensor(self.n_feats, self.n_feats))\n",
        "        self.w_j = nn.Parameter(torch.Tensor(self.n_feats, self.n_feats))\n",
        "        self.a = nn.Parameter(torch.Tensor(1, self.n_feats))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.n_feats))\n",
        "\n",
        "        self.edge_emb = nn.Sequential(nn.Linear(edge_feats, self.n_feats))\n",
        "\n",
        "        self.lin1 = nn.Sequential(nn.BatchNorm1d(n_feats), nn.Linear(n_feats, self.snd_n_feats))\n",
        "        self.lin2 = nn.Sequential(nn.BatchNorm1d(self.snd_n_feats), CustomDropout(self.dropout), nn.PReLU(), nn.Linear(self.snd_n_feats, self.snd_n_feats))\n",
        "        self.lin3 = nn.Sequential(nn.BatchNorm1d(self.snd_n_feats), CustomDropout(self.dropout), nn.PReLU(), nn.Linear(self.snd_n_feats, self.snd_n_feats))\n",
        "        self.lin4 = nn.Sequential(nn.BatchNorm1d(self.snd_n_feats), CustomDropout(self.dropout), nn.PReLU(), nn.Linear(self.snd_n_feats, self.snd_n_feats))\n",
        "\n",
        "        glorot(self.w_i)\n",
        "        glorot(self.w_j)\n",
        "        glorot(self.a)\n",
        "\n",
        "        self.sml_mlp = nn.Sequential(nn.PReLU(), nn.Linear(self.n_feats, self.n_feats))\n",
        "\n",
        "    def forward(self, data):\n",
        "        edge_index = data.edge_index\n",
        "        edge_feats = data.edge_feats\n",
        "        edge_feats = self.edge_emb(edge_feats)\n",
        "        deg = degree(edge_index[1], data.x.size(0), dtype=data.x.dtype)\n",
        "\n",
        "        alpha_i = (data.x @ self.w_i)\n",
        "        alpha_j = (data.x @ self.w_j)\n",
        "        alpha = alpha_i[edge_index[1]] + alpha_j[edge_index[0]] + self.bias\n",
        "        alpha = self.sml_mlp(alpha)\n",
        "\n",
        "        alpha = (alpha * edge_feats).sum(-1)\n",
        "        alpha = alpha / (deg[edge_index[0]])\n",
        "        edge_weights = torch.sigmoid(alpha)\n",
        "\n",
        "        edge_attr = data.x[edge_index[0]] * edge_weights.unsqueeze(-1)\n",
        "\n",
        "        out = edge_attr\n",
        "        for _ in range(self.n_iter):\n",
        "            out = scatter(out[data.line_graph_edge_index[0]], data.line_graph_edge_index[1], dim_size=edge_attr.size(0), dim=0, reduce='add')\n",
        "            out = edge_attr + (out * edge_weights.unsqueeze(-1))\n",
        "\n",
        "        x = data.x + scatter(out, edge_index[1], dim_size=data.x.size(0), dim=0, reduce='add')\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def mlp(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = (self.lin3(self.lin2(x)) + x) / 2\n",
        "        x = (self.lin4(x) + x) / 2\n",
        "        return x\n",
        "\n",
        "# Define GmpnnCSNetDrugBank\n",
        "class GmpnnCSNetDrugBank(nn.Module):\n",
        "    def __init__(self, in_feats, edge_feats, hid_feats, rel_total, n_iter, dropout=0):\n",
        "        super().__init__()\n",
        "        self.in_feats = in_feats\n",
        "        self.hid_feats = hid_feats\n",
        "        self.rel_total = rel_total\n",
        "        self.n_iter = n_iter\n",
        "        self.dropout = dropout\n",
        "        self.snd_hid_feats = hid_feats * 2\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_feats, hid_feats),\n",
        "            CustomDropout(self.dropout),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(hid_feats, hid_feats),\n",
        "            nn.BatchNorm1d(hid_feats),\n",
        "            CustomDropout(self.dropout),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(hid_feats, hid_feats),\n",
        "            nn.BatchNorm1d(hid_feats),\n",
        "            CustomDropout(self.dropout),\n",
        "        )\n",
        "\n",
        "        self.propagation_layer = GmpnnBlock(edge_feats, self.hid_feats, self.n_iter, dropout)\n",
        "\n",
        "        self.i_pro = nn.Parameter(torch.zeros(self.snd_hid_feats, self.hid_feats))\n",
        "        self.j_pro = nn.Parameter(torch.zeros(self.snd_hid_feats, self.hid_feats))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.hid_feats))\n",
        "\n",
        "        self.rel_embs = nn.Embedding(self.rel_total, self.hid_feats)\n",
        "\n",
        "        glorot(self.i_pro)\n",
        "        glorot(self.j_pro)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        drug_data, unique_drug_pair, rels, drug_pair_indices, node_j_for_pairs, node_i_for_pairs = batch\n",
        "        drug_data.x = self.mlp(drug_data.x)\n",
        "\n",
        "        new_feats = self.propagation_layer(drug_data)\n",
        "        drug_data.x = new_feats\n",
        "        x_j = drug_data.x[node_j_for_pairs]\n",
        "        x_i = drug_data.x[node_i_for_pairs]\n",
        "\n",
        "        pair_repr = ((x_i[unique_drug_pair.edge_index[1]] @ self.i_pro) * (x_j[unique_drug_pair.edge_index[0]] @ self.j_pro))\n",
        "        pair_repr = scatter(pair_repr, unique_drug_pair.edge_index_batch, reduce='add', dim=0)[drug_pair_indices]\n",
        "\n",
        "        p_scores, n_scores = self.compute_score(pair_repr, rels)\n",
        "        return p_scores, n_scores\n",
        "\n",
        "    def compute_score(self, pair_repr, rels):\n",
        "        batch_size = len(rels)\n",
        "        neg_n = (len(pair_repr) - batch_size) // batch_size\n",
        "        rels = torch.cat([rels, torch.repeat_interleave(rels, neg_n, dim=0)], dim=0)\n",
        "        rels = self.rel_embs(rels)\n",
        "        scores = (pair_repr * rels).sum(-1)\n",
        "        p_scores, n_scores = scores[:batch_size].unsqueeze(-1), scores[batch_size:].view(batch_size, -1, 1)\n",
        "        return p_scores, n_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-5ovqLkr0Jz",
        "outputId": "28c1816c-e1e8-439b-eae5-2e72a7da7ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GmpnnCSNetDrugBank(\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): CustomDropout()\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): CustomDropout()\n",
            "    (6): PReLU(num_parameters=1)\n",
            "    (7): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): CustomDropout()\n",
            "  )\n",
            "  (propagation_layer): GmpnnBlock(\n",
            "    (edge_emb): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
            "    )\n",
            "    (lin1): Sequential(\n",
            "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
            "    )\n",
            "    (lin2): Sequential(\n",
            "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): CustomDropout()\n",
            "      (2): PReLU(num_parameters=1)\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (lin3): Sequential(\n",
            "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): CustomDropout()\n",
            "      (2): PReLU(num_parameters=1)\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (lin4): Sequential(\n",
            "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): CustomDropout()\n",
            "      (2): PReLU(num_parameters=1)\n",
            "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (sml_mlp): Sequential(\n",
            "      (0): PReLU(num_parameters=1)\n",
            "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (rel_embs): Embedding(86, 64)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = GmpnnCSNetDrugBank(in_feats=128, edge_feats=32, hid_feats=64, rel_total=86, n_iter=2)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOWLwTCUoiSD"
      },
      "source": [
        "# Train on Fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dhi2GOopumy7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class SigmoidLoss(nn.Module):\n",
        "\n",
        "    def forward(self, p_scores, n_scores):\n",
        "        p_loss = - F.logsigmoid(p_scores).mean()\n",
        "        n_loss = - F.logsigmoid(-n_scores).mean()\n",
        "\n",
        "        return (p_loss + n_loss) / 2, p_loss, n_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "xeW-3yQQurIb"
      },
      "outputs": [],
      "source": [
        "from operator import le\n",
        "from sklearn import metrics\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def do_compute_metrics(probas_pred, target):\n",
        "    pred = (probas_pred >= 0.5).astype(int)\n",
        "    acc = metrics.accuracy_score(target, pred)\n",
        "    auroc = metrics.roc_auc_score(target, probas_pred)\n",
        "    f1_score = metrics.f1_score(target, pred)\n",
        "    precision = metrics.precision_score(target, pred)\n",
        "    recall = metrics.recall_score(target, pred)\n",
        "    p, r, t = metrics.precision_recall_curve(target, probas_pred)\n",
        "    int_ap = metrics.auc(r, p)\n",
        "    ap= metrics.average_precision_score(target, probas_pred)\n",
        "\n",
        "    return acc, auroc, f1_score, precision, recall, int_ap, ap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "v2fu72YEl4Uc",
        "outputId": "42548010-e0c9-4ba5-f9ed-1a16645c66a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading drugbank...\n",
            "\n",
            "Loading processed drug data...\n",
            "\n",
            "Loading data/preprocessed/drugbank/pair_pos_neg_triplets_train_fold0.csv...\n",
            "\n",
            "Loading data/preprocessed/drugbank/pair_pos_neg_triplets_test_fold0.csv...\n",
            "1 negative samples on fold 0\n",
            "\n",
            "Training on 122750 samples, validating on 30688, and testing on 38360 samples.\n",
            "Training on cuda.\n",
            "Starting fold_0 at 2025-02-28 16:11:09.772483\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train Epoch 1:   0%|          | 0/3836 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "train Epoch 1: 100%|██████████| 3836/3836 [04:31<00:00, 14.10it/s]  \n",
            "val Epoch 1:   0%|          | 0/959 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "val Epoch 1: 100%|██████████| 959/959 [00:58<00:00, 16.44it/s]\n",
            "test Epoch 1:   0%|          | 0/1199 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "test Epoch 1: 100%|██████████| 1199/1199 [01:16<00:00, 15.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#### Epoch time 407.5823s\n",
            "loss: 2.1496, acc: 0.5063, roc: 0.5074, f1: 0.5079, p: 0.5062, r: 0.5095, int-ap: 0.5113, ap: 0.5061\n",
            "#### Validation\n",
            "loss: 0.7009, acc: 0.5347, roc: 0.5523, f1: 0.5294, p: 0.5354, r: 0.5236, int-ap: 0.5401, ap: 0.5402\n",
            "#### Test\n",
            "loss: 0.6994, acc: 0.5306, roc: 0.5498, f1: 0.5242, p: 0.5314, r: 0.5172, int-ap: 0.5379, ap: 0.5379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train Epoch 2:   0%|          | 0/3836 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "train Epoch 2: 100%|██████████| 3836/3836 [03:56<00:00, 16.22it/s]\n",
            "val Epoch 2:   0%|          | 0/959 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "val Epoch 2: 100%|██████████| 959/959 [00:58<00:00, 16.38it/s]\n",
            "test Epoch 2:   0%|          | 0/1199 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "test Epoch 2: 100%|██████████| 1199/1199 [01:11<00:00, 16.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#### Epoch time 367.2161s\n",
            "loss: 0.7241, acc: 0.5359, roc: 0.5499, f1: 0.5490, p: 0.5339, r: 0.5649, int-ap: 0.5404, ap: 0.5404\n",
            "#### Validation\n",
            "loss: 0.6769, acc: 0.5928, roc: 0.6327, f1: 0.6551, p: 0.5682, r: 0.7735, int-ap: 0.6053, ap: 0.6054\n",
            "#### Test\n",
            "loss: 0.6748, acc: 0.5927, roc: 0.6334, f1: 0.6550, p: 0.5681, r: 0.7734, int-ap: 0.6079, ap: 0.6079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train Epoch 3:   0%|          | 0/3836 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "train Epoch 3: 100%|██████████| 3836/3836 [04:04<00:00, 15.69it/s] \n",
            "val Epoch 3:   0%|          | 0/959 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "val Epoch 3: 100%|██████████| 959/959 [00:58<00:00, 16.49it/s]\n",
            "test Epoch 3:   0%|          | 0/1199 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "test Epoch 3: 100%|██████████| 1199/1199 [01:13<00:00, 16.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#### Epoch time 376.7321s\n",
            "loss: 0.6628, acc: 0.6035, roc: 0.6437, f1: 0.6172, p: 0.5967, r: 0.6391, int-ap: 0.6247, ap: 0.6247\n",
            "#### Validation\n",
            "loss: 0.6530, acc: 0.6247, roc: 0.6749, f1: 0.6792, p: 0.5931, r: 0.7947, int-ap: 0.6416, ap: 0.6416\n",
            "#### Test\n",
            "loss: 0.6523, acc: 0.6244, roc: 0.6743, f1: 0.6786, p: 0.5930, r: 0.7932, int-ap: 0.6425, ap: 0.6425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train Epoch 4:   0%|          | 0/3836 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "train Epoch 4: 100%|██████████| 3836/3836 [03:56<00:00, 16.25it/s]\n",
            "val Epoch 4:   0%|          | 0/959 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "val Epoch 4: 100%|██████████| 959/959 [01:08<00:00, 13.91it/s]\n",
            "test Epoch 4:   0%|          | 0/1199 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "test Epoch 4: 100%|██████████| 1199/1199 [00:49<00:00, 24.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#### Epoch time 355.3553s\n",
            "loss: 0.6390, acc: 0.6340, roc: 0.6871, f1: 0.6434, p: 0.6274, r: 0.6602, int-ap: 0.6663, ap: 0.6663\n",
            "#### Validation\n",
            "loss: 0.6262, acc: 0.6519, roc: 0.7096, f1: 0.6768, p: 0.6316, r: 0.7289, int-ap: 0.6870, ap: 0.6870\n",
            "#### Test\n",
            "loss: 0.6254, acc: 0.6512, roc: 0.7092, f1: 0.6745, p: 0.6322, r: 0.7229, int-ap: 0.6892, ap: 0.6892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train Epoch 5:   0%|          | 0/3836 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "train Epoch 5: 100%|██████████| 3836/3836 [04:53<00:00, 13.09it/s]\n",
            "val Epoch 5:   0%|          | 0/959 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "val Epoch 5: 100%|██████████| 959/959 [01:03<00:00, 15.07it/s]\n",
            "test Epoch 5:   0%|          | 0/1199 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "test Epoch 5: 100%|██████████| 1199/1199 [02:15<00:00,  8.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#### Epoch time 492.7232s\n",
            "loss: 0.6182, acc: 0.6561, roc: 0.7168, f1: 0.6656, p: 0.6477, r: 0.6845, int-ap: 0.6974, ap: 0.6974\n",
            "#### Validation\n",
            "loss: 0.6146, acc: 0.6621, roc: 0.7227, f1: 0.6980, p: 0.6310, r: 0.7810, int-ap: 0.7025, ap: 0.7025\n",
            "#### Test\n",
            "loss: 0.6149, acc: 0.6621, roc: 0.7232, f1: 0.6969, p: 0.6319, r: 0.7768, int-ap: 0.7032, ap: 0.7032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train Epoch 6:   0%|          | 0/3836 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "train Epoch 6: 100%|██████████| 3836/3836 [16:16<00:00,  3.93it/s]   \n",
            "val Epoch 6:   0%|          | 0/959 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "val Epoch 6: 100%|██████████| 959/959 [00:47<00:00, 20.40it/s]\n",
            "test Epoch 6:   0%|          | 0/1199 [00:00<?, ?it/s]c:\\Users\\swath\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'j_indices', 'edge_index', 'i_indices'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n",
            "test Epoch 6:  79%|███████▉  | 953/1199 [05:04<00:42,  5.81it/s]"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "dataset_name = 'drugbank'\n",
        "fold_i = 0\n",
        "dropout = 0.2\n",
        "n_iter = 3\n",
        "TOTAL_NUM_RELS = total_num_rel()\n",
        "batch_size = 512\n",
        "data_size_ratio = 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "hid_feats = 64\n",
        "rel_total = TOTAL_NUM_RELS\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-4\n",
        "n_epochs = 20\n",
        "kge_feats = 64\n",
        "\n",
        "def do_compute(model, batch, device):\n",
        "\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        p_score, n_score = model(batch)\n",
        "        assert p_score.ndim == 2\n",
        "        assert n_score.ndim == 3\n",
        "        probas_pred = np.concatenate([torch.sigmoid(p_score.detach()).cpu().mean(dim=-1), torch.sigmoid(n_score.detach()).mean(dim=-1).view(-1).cpu()])\n",
        "        ground_truth = np.concatenate([np.ones(p_score.shape[0]), np.zeros(n_score.shape[:2]).reshape(-1)])\n",
        "\n",
        "        return p_score, n_score, probas_pred, ground_truth\n",
        "\n",
        "\n",
        "def run_batch(model, optimizer, data_loader, epoch_i, desc, loss_fn, device):\n",
        "        total_loss = 0\n",
        "        loss_pos = 0\n",
        "        loss_neg = 0\n",
        "        probas_pred = []\n",
        "        ground_truth = []\n",
        "\n",
        "        for batch in tqdm(data_loader, desc= f'{desc} Epoch {epoch_i}'):\n",
        "            p_score, n_score, batch_probas_pred, batch_ground_truth = do_compute(model, batch, device)\n",
        "\n",
        "            probas_pred.append(batch_probas_pred)\n",
        "            ground_truth.append(batch_ground_truth)\n",
        "\n",
        "            loss, loss_p, loss_n = loss_fn(p_score, n_score)\n",
        "            if model.training:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss_pos += loss_p.item()\n",
        "            loss_neg += loss_n.item()\n",
        "        total_loss /= len(data_loader)\n",
        "        loss_pos /= len(data_loader)\n",
        "        loss_neg /= len(data_loader)\n",
        "\n",
        "        probas_pred = np.concatenate(probas_pred)\n",
        "        ground_truth = np.concatenate(ground_truth)\n",
        "\n",
        "        return total_loss, do_compute_metrics(probas_pred, ground_truth)\n",
        "\n",
        "\n",
        "def print_metrics(loss, acc, auroc, f1_score, precision, recall, int_ap, ap):\n",
        "    print(f'loss: {loss:.4f}, acc: {acc:.4f}, roc: {auroc:.4f}, f1: {f1_score:.4f}, ', end='')\n",
        "    print(f'p: {precision:.4f}, r: {recall:.4f}, int-ap: {int_ap:.4f}, ap: {ap:.4f}')\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "\n",
        "def train(model, train_data_loader, val_data_loader, test_data_loader, loss_fn, optimizer, n_epochs, device, scheduler):\n",
        "    for epoch_i in range(1, n_epochs+1):\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        \n",
        "        ## Training\n",
        "        train_loss, train_metrics = run_batch(model, optimizer, train_data_loader, epoch_i, 'train', loss_fn, device)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            ## Validation\n",
        "            if val_data_loader:\n",
        "                val_loss, val_metrics = run_batch(model, optimizer, val_data_loader, epoch_i, 'val', loss_fn, device)\n",
        "            \n",
        "            ## Test Set Evaluation\n",
        "            if test_data_loader:\n",
        "                test_loss, test_metrics = run_batch(model, optimizer, test_data_loader, epoch_i, 'test', loss_fn, device)\n",
        "\n",
        "        print(f'\\n#### Epoch time {time.time() - start:.4f}s')\n",
        "        print_metrics(train_loss, *train_metrics)\n",
        "\n",
        "        if val_data_loader:\n",
        "            print('#### Validation')\n",
        "            print_metrics(val_loss, *val_metrics)\n",
        "\n",
        "        if test_data_loader:\n",
        "            print('#### Test')\n",
        "            print_metrics(test_loss, *test_metrics)\n",
        "\n",
        "\n",
        "\n",
        "train_data_loader, val_data_loader, test_data_loader, NUM_FEATURES, NUM_EDGE_FEATURES = \\\n",
        "    load_ddi_data_fold(\n",
        "    fold=0, batch_size=32, data_size_ratio=1.0)\n",
        "\n",
        "GmpnnNet = GmpnnCSNetDrugBank if dataset_name == 'drugbank' else GmpnnCSNetDrugBank\n",
        "\n",
        "model = GmpnnNet(NUM_FEATURES, NUM_EDGE_FEATURES, hid_feats, rel_total, n_iter, dropout)\n",
        "loss_fn = SigmoidLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.96 ** (epoch))\n",
        "\n",
        "time_stamp = f'{datetime.now()}'.replace(':', '_')\n",
        "\n",
        "\n",
        "model.to(device=device)\n",
        "print(f'Training on {device}.')\n",
        "print(f'Starting fold_{fold_i} at', datetime.now())\n",
        "train(model, train_data_loader, val_data_loader, test_data_loader, loss_fn, optimizer, n_epochs, device, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
